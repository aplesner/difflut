{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fddd7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:7: RuntimeWarning: CUDA extension 'efd_cuda' not available. DWNNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.dwn_node')\n",
      "  from .dwn_node import DWNNode\n",
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:9: RuntimeWarning: CUDA extension 'probabilistic_cuda' not available. ProbabilisticNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.probabilistic_node')\n",
      "  from .probabilistic_node import ProbabilisticNode\n",
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:10: RuntimeWarning: CUDA extension 'probabilistic_stable_cuda' not available. ProbabilisticStableNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.probabilistic_stable_node')\n",
      "  from .probabilistic_stable_node import ProbabilisticStableNode\n",
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:13: RuntimeWarning: CUDA extension 'hybrid_cuda' not available. HybridNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.hybrid_node')\n",
      "  from .hybrid_node import HybridNode\n",
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:14: RuntimeWarning: CUDA extension 'fourier_cuda' not available. FourierNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.fourier_node')\n",
      "  from .fourier_node import FourierNode\n",
      "/home/aplesner/code/difflut/difflut/nodes/__init__.py:15: RuntimeWarning: CUDA extension 'dwn_stable_cuda' not available. DWNStableNode will use slower CPU fallback. For better performance, compile the CUDA extension using: 'cd difflut && python setup.py install'. To suppress this warning: warnings.filterwarnings('ignore', category=RuntimeWarning, module='difflut.nodes.dwn_stable_node')\n",
      "  from .dwn_stable_node import DWNStableNode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from difflut.models.feedforward import feedforward_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42a02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLUTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional layer using LUT-based nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            tree_depth: int,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            receptive_field: int | tuple[int, int] = 5,\n",
    "            stride: int | tuple[int, int] = 1,\n",
    "            padding: int | tuple[int, int] = 0,\n",
    "            node_type: str = 'dwn',\n",
    "            layer_type: str = 'random',\n",
    "            n_inputs_per_node: int = 6,\n",
    "            # node_kwargs: dict | None = None\n",
    "            ):\n",
    "        super(ConvolutionalLUTLayer, self).__init__()\n",
    "        self.tree_depth = tree_depth\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.receptive_field = self._pair(receptive_field)\n",
    "        self.input_size = in_channels * self.receptive_field[0] * self.receptive_field[1]\n",
    "        self.stride = self._pair(stride)\n",
    "        self.padding = self._pair(padding)\n",
    "        self.node_type = node_type\n",
    "        self.layer_type = layer_type\n",
    "        self.n_inputs_per_node = n_inputs_per_node\n",
    "        # self.node_kwargs = node_kwargs if node_kwargs is not None else {}\n",
    "\n",
    "        # Create trees (one for each output channel)\n",
    "        # Each tree is a small feedforward network of LUT nodes\n",
    "        hidden_layers = [self.n_inputs_per_node ** (self.tree_depth - i) for i in range(self.tree_depth + 1)]\n",
    "        self.trees = nn.ModuleList()\n",
    "        for _ in range(out_channels):\n",
    "            tree = feedforward_core(\n",
    "                input_size=self.input_size,\n",
    "                hidden_sizes=hidden_layers,\n",
    "                node_type=self.node_type,\n",
    "                layer_type=self.layer_type,\n",
    "                n_inputs=self.n_inputs_per_node,\n",
    "                # node_kwargs=self.node_kwargs\n",
    "            )\n",
    "            self.trees.append(tree)\n",
    "        \n",
    "        # For convolution, we use the unfold operation\n",
    "        self.unfold = nn.Unfold(kernel_size=receptive_field, padding=0, stride=1)\n",
    "\n",
    "    def _pair(self, x: int | tuple[int, int]) -> tuple[int, int]:\n",
    "        if isinstance(x, int):\n",
    "            return (x, x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Extract patches: (batch, patch_size, num_patches)\n",
    "        patches = self.unfold(x)\n",
    "        num_patches = patches.shape[2]\n",
    "        print(\"Patches shape:\", patches.shape)\n",
    "        \n",
    "        # Reshape to (batch*num_patches, patch_size)\n",
    "        patches = patches.transpose(1, 2).contiguous()\n",
    "        print(\"Patches shape:\", patches.shape)\n",
    "        patches = patches.view(-1, self.input_size)\n",
    "        print(\"Patches shape:\", patches.shape)\n",
    "\n",
    "        # Process each patch through each tree\n",
    "        output = [tree(patches) for tree in self.trees]\n",
    "        output = torch.stack(output, dim=1)  # (batch*num_patches, out_channels)\n",
    "\n",
    "        output = output.view(batch_size, num_patches, self.out_channels)\n",
    "        output = output.transpose(1, 2)  # (batch, out_channels, num_patches)\n",
    "        \n",
    "        # Calculate output spatial dimensions\n",
    "        out_h = (x.shape[2] + 2 * self.padding[0] - self.receptive_field[0]) // self.stride[0] + 1\n",
    "        out_w = (x.shape[3] + 2 * self.padding[1] - self.receptive_field[1]) // self.stride[1] + 1\n",
    "\n",
    "        output = output.view(batch_size, self.out_channels, out_h, out_w)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f45448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DiffLUT model:\n",
      "  Node type: dwn\n",
      "  Input size: 25\n",
      "  Hidden layers: [216, 36, 6, 1]\n",
      "  Layer 1: 25 → 216\n",
      "  Layer 2: 216 → 36\n",
      "  Layer 3: 36 → 6\n",
      "  Layer 4: 6 → 1\n",
      "\n",
      "Building DiffLUT model:\n",
      "  Node type: dwn\n",
      "  Input size: 25\n",
      "  Hidden layers: [216, 36, 6, 1]\n",
      "  Layer 1: 25 → 216\n",
      "  Layer 2: 216 → 36\n",
      "  Layer 3: 36 → 6\n",
      "  Layer 4: 6 → 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aplesner/code/difflut/difflut/layers/base_layer.py:26: RuntimeWarning: DWNNode: CUDA was requested (use_cuda=True) but CUDA extension is not available. Using CPU fallback which may be significantly slower. To enable CUDA: compile the extension with 'cd difflut && python setup.py install'\n",
      "  node = node_type(input_dim=[n], output_dim=[1], **node_kwargs_i)\n"
     ]
    }
   ],
   "source": [
    "conv_lut_layer = ConvolutionalLUTLayer(\n",
    "    tree_depth=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    receptive_field=5,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    node_type='dwn',\n",
    "    layer_type='random',\n",
    "    n_inputs_per_node=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8523ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape: torch.Size([2, 25, 576])\n",
      "Patches shape: torch.Size([2, 576, 25])\n",
      "Patches shape: torch.Size([1152, 25])\n",
      "Output shape: torch.Size([2, 2, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "image_size = 28\n",
    "# get some random binary input data\n",
    "input_data = torch.randint(0, 2, (batch_size, 1, image_size, image_size)).float()\n",
    "\n",
    "output = conv_lut_layer(input_data)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6031534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
