
# DiffLUT Library
![Python](https://img.shields.io/badge/python-3.7%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
[![arXiv](https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg)]()

DiffLUT is a modular PyTorch library for differentiable Look-Up Table (LUT) neural networks, designed for efficient FPGA deployment and research. It provides encoders, LUT nodes, flexible layers, and CUDA acceleration.

## âœ¨ Features

- **Modular Architecture**: Encoders, LUT nodes, and layers as composable building blocks
- **Multiple Node Types**: LinearLUT, PolyLUT, NeuralLUT, DWN, DWNStable, Probabilistic, Fourier, Hybrid
- **Input Encoders**: Thermometer, Gaussian, Distributive, Gray, OneHot, Binary, SignMagnitude, Logarithmic
- **Flexible Layers**: Random and Learnable connectivity patterns
- **CUDA Acceleration**: Optional GPU support for compute-intensive nodes
- **FPGA Export**: Tools for deploying trained networks to FPGAs
- **Component Registry**: Easy discovery and instantiation of components

## ðŸš€ Quick Links

### For Users
- **[Installation Guide](docs/INSTALLATION.md)** - Setup and requirements
- **[Quick Start](docs/QUICK_START.md)** - Get running in 5 minutes
- **[User Guide](docs/USER_GUIDE.md)** - Learn the library components and patterns
  - [Components Guide](docs/USER_GUIDE/components.md) - Encoders, nodes, and layers
  - [Registry & Pipelines](docs/USER_GUIDE/registry_pipeline.md) - Component discovery and pipeline building

### For Developers
- **[Developer Guide](docs/DEVELOPER_GUIDE.md)** - Extend and contribute to DiffLUT
  - [Creating Components](docs/DEVELOPER_GUIDE/creating_components.md) - Implement custom nodes/encoders/layers
  - [Packaging & Distribution](docs/DEVELOPER_GUIDE/packaging.md) - Build and publish DiffLUT
  - [Contributing](docs/DEVELOPER_GUIDE/contributing.md) - Development setup and guidelines
  - [Tests](docs/DEVELOPER_GUIDE/tests.md) - Test setup and guidelines

## ðŸ“¦ Package Structure

DiffLUT is organized as a self-contained Python package:

```
difflut/                       # Top-level package directory
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ setup.py                   # Installation script
â”œâ”€â”€ pyproject.toml             # Build system config
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ difflut/                   # Main library code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ registry.py            # Component registration utilities
â”‚   â”œâ”€â”€ encoder/               # Input encoders
â”‚   â”œâ”€â”€ layers/                # Layer types
â”‚   â”œâ”€â”€ nodes/                 # LUT node implementations
â”‚   â”‚   â””â”€â”€ utils/             # initializers and regularizer
â”‚   â””â”€â”€ utils/                 # Utility functions (FPGA export)
â”œâ”€â”€ examples/                  # Example scripts and notebooks
â””â”€â”€ tests/                     # Test suite
```

## ðŸ“š Citation

If you use DiffLUT in your research, please cite:

```bibtex
@software{difflut2025,
  title={DiffLUT: Differentiable Lookup Table Networks},
  author={B\"uhrer, Simon Jonas and Plesner, Andreas and Aczel, Till},
  year={2025},
  institution={ETH Zurich, Distributed Computing Group},
  url={https://github.com/aplesner/difflut}
}
```

## ðŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

## ðŸ‘¤ Contact

- **Authors**: Simon Jonas BÃ¼hrer, Andreas Plesner, and Till Aczel
- **Emails**: {sbuehrer,aplesner,taczel}@ethz.ch
- **Institution**: ETH Zurich, Distributed Computing Group
- **Issues**: https://github.com/aplesner/difflut/-/issues

## ðŸ”— Related Projects

Part of the **DiffLUT Research Framework**:
- **Parent Project**: Full experiment pipeline with Hydra configs and SLURM scripts
